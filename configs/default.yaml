# Crawler settings
crawler:
  workers: 5              # Number of concurrent workers
  rate_limit: 500ms       # Milliseconds between requests (per worker)
  timeout: 30s            # Request timeout in seconds
  max_depth: 10           # Maximum crawl depth from seed URL
  max_pages: 1000         # Maximum number of pages to crawl (0 for unlimited)

# MongoDB settings
storage:
  mongodb:
    database: "webcrawler"
    collection: "webpages"
    timeout: 30s          # Connection timeout
    max_pool_size: 50     # Maximum number of connections in the pool
    min_pool_size: 10     # Minimum number of connections to maintain
    max_idle_time: 5m     # Maximum time a connection can remain idle

# HTTP client settings
http:
  user_agent: "GoWebCrawler/1.0"
  follow_redirects: true
  max_redirects: 10
  timeout: 30s           # Request timeout in seconds
  retry:
    max_attempts: 3
    initial_delay: 1s    # Initial retry delay
    max_delay: 5s        # Maximum retry delay

# URL filtering
filters:
  allowed_domains: []    # Empty means same domain as seed
  excluded_paths:        # Paths to skip
    - "/wp-admin"
    - "/wp-login"
    - "/wp-content"
    - "/admin"
    - "/login"
  allowed_schemes:       # URL schemes to allow
    - "http"
    - "https"
  excluded_extensions:   # File extensions to skip
    - ".pdf"
    - ".jpg"
    - ".jpeg"
    - ".png"
    - ".gif"
    - ".zip"
    - ".tar"
    - ".gz"
    - ".rar"
    - ".exe"
    - ".doc"
    - ".docx"
    - ".xls"
    - ".xlsx"
    - ".ppt"
    - ".pptx"

benchmark:
  enabled: true
  interval: 1s           # Metric collection interval
  output_dir: "benchmarks" 