# High-Performance Crawler Configuration

# Crawler settings - Optimized for performance
crawler:
  workers: 20             # Increased workers for higher concurrency (was 5)
  rate_limit: 100ms       # Much faster rate limiting (was 500ms)
  timeout: 15s            # Faster timeout for responsive servers
  max_depth: 10           # Maximum crawl depth from seed URL
  max_pages: 5000         # Higher page limit for testing (was 1000)

# MongoDB settings (optional - can work without MongoDB)
storage:
  mongodb:
    database: "webcrawler"
    collection: "webpages"
    timeout: 15s          # Faster timeout
    max_pool_size: 100    # Higher connection pool
    min_pool_size: 20     # Higher minimum pool
    max_idle_time: 3m     # Shorter idle time

# HTTP client settings - Optimized for performance
http:
  user_agent: "HighPerformanceWebCrawler/2.0"
  follow_redirects: true
  max_redirects: 5        # Reduced from 10 for speed
  timeout: 15s            # Faster timeout (was 30s)

# URL filtering settings
filters:
  allowed_schemes: ["http", "https"]
  allowed_domains: []     # Empty = same domain only
  excluded_paths: [
    "/wp-admin/", "/admin/", "/login/", "/logout/",
    "/api/", "/ajax/", "/search/", "/feed/",
    "/rss/", "/atom/", "/.git/", "/node_modules/"
  ]
  excluded_extensions: [
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
    ".zip", ".rar", ".tar", ".gz", ".7z",
    ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".svg", ".webp",
    ".mp3", ".mp4", ".avi", ".mov", ".wmv", ".flv",
    ".css", ".js", ".json", ".xml", ".ico", ".woff", ".woff2", ".ttf", ".eot"
  ]

# Benchmarking settings
benchmark:
  enabled: true
  interval: 1s            # Record metrics every second
  output_dir: "benchmarks" 