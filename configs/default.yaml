# Crawler settings
crawler:
  workers: 5              # Number of concurrent workers
  rate_limit: 500         # Milliseconds between requests (per worker)
  timeout: 30             # Request timeout in seconds
  max_depth: 10           # Maximum crawl depth from seed URL
  max_pages: 1000         # Maximum number of pages to crawl (0 for unlimited)

# MongoDB settings
storage:
  mongodb:
    database: "webcrawler"
    collection: "webpages"
    timeout: 10           # Connection timeout in seconds
    max_pool_size: 100    # Maximum number of connections

# HTTP client settings
http:
  user_agent: "GoWebCrawler/1.0"
  follow_redirects: true
  max_redirects: 10
  timeout: 30            # Request timeout in seconds
  retry:
    max_attempts: 3
    initial_delay: 1     # Seconds
    max_delay: 5         # Seconds

# URL filtering
filters:
  allowed_domains: []    # Empty means same domain as seed
  excluded_paths:        # Paths to skip
    - "/wp-admin"
    - "/wp-login"
    - "/wp-content"
    - "/admin"
    - "/login"
  allowed_schemes:       # URL schemes to allow
    - "http"
    - "https"
  excluded_extensions:   # File extensions to skip
    - ".pdf"
    - ".jpg"
    - ".jpeg"
    - ".png"
    - ".gif"
    - ".zip"
    - ".tar"
    - ".gz"
    - ".rar"
    - ".exe"
    - ".doc"
    - ".docx"
    - ".xls"
    - ".xlsx"
    - ".ppt"
    - ".pptx" 